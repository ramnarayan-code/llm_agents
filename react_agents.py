# -*- coding: utf-8 -*-
"""agent_creator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bbXEmD1oOyF82Aj7kBFm35dRI-E3-G4P
"""

!pip install -U langgraph langchain langchain-openai langchain-community faiss-cpu

import langgraph

import os

os.environ["OPENAI_API_KEY"] = "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"

import sqlite3

def create_connection(db_file):
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        return conn
    except sqlite3.Error as e:
        print(e)
    return conn

def create_table(conn, create_table_sql):
    try:
        c = conn.cursor()
        c.execute(create_table_sql)
    except sqlite3.Error as e:
        print(e)

def insert_employee(conn, employee):
    sql = ''' INSERT INTO employee(id, name, designation)
              VALUES(?,?,?) '''
    cur = conn.cursor()
    cur.execute(sql, employee)
    conn.commit()
    return cur.lastrowid

def insert_reporting(conn, reporting):
    sql = ''' INSERT INTO reporting(id, manager_id)
              VALUES(?,?) '''
    cur = conn.cursor()
    cur.execute(sql, reporting)
    conn.commit()
    return cur.lastrowid

def main():
    database = "test.db"

    sql_create_employee_table = """ CREATE TABLE IF NOT EXISTS employee (
                                        id integer PRIMARY KEY,
                                        name text NOT NULL,
                                        designation text NOT NULL
                                    ); """

    sql_create_reporting_table = """ CREATE TABLE IF NOT EXISTS reporting (
                                        id integer NOT NULL,
                                        manager_id integer,
                                        FOREIGN KEY (id) REFERENCES employee (id)
                                    ); """

    conn = create_connection(database)

    if conn is not None:
        create_table(conn, sql_create_employee_table)
        create_table(conn, sql_create_reporting_table)

        employees = [(1, 'Alice', 'Manager'),
                     (2, 'Bob', 'Developer'),
                     (3, 'Charlie', 'Developer'),
                     (4, 'David', 'Analyst'),
                     (5, 'Eve', 'Designer')]

        reportings = [(2, 1),  # Bob reports to Alice
                      (3, 1),  # Charlie reports to Alice
                      (4, 1),  # David reports to Alice
                      (5, 1)]  # Eve reports to Alice

        for employee in employees:
            insert_employee(conn, employee)

        for reporting in reportings:
            insert_reporting(conn, reporting)

        print("Database created and tables populated successfully.")

        emp_dict = get_employees(conn)
        print("Employee details with reporting structure:")
        print(emp_dict)

    else:
        print("Error! Cannot create the database connection.")

if __name__ == '__main__':
    main()

@tool
def get_employees():
  """Gets the employees list."""
  conn = create_connection("test.db")
  employees = {}
  try:
      cur = conn.cursor()
      cur.execute("""
          SELECT e.id, e.name, e.designation, r.manager_id, m.name as manager_name
          FROM employee e
          LEFT JOIN reporting r ON e.id = r.id
          LEFT JOIN employee m ON r.manager_id = m.id
      """)
      rows = cur.fetchall()
      for row in rows:
          emp_id, name, designation, manager_id, manager_name = row
          employees[emp_id] = {
              "name": name,
              "designation": designation,
              "manager_id": manager_id,
              "manager_name": manager_name
          }
  except sqlite3.Error as e:
      print(e)
  return employees

from langchain_core.messages import SystemMessage
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

class Agent:
  def __init__(self, prompt, tools, model):
    self.__system_message = prompt
    self.__tools = tools
    self.__model = model
    self.__memory = MemorySaver()

  def create(self):
    self.__agent = create_react_agent(self.__model, self.__tools, state_modifier=self.__system_message, checkpointer=self.__memory)

  def invoke(self, input, config=None):
    return self.__agent.invoke({"messages": input}, config)["messages"][-1].content

model = ChatOpenAI(model="gpt-4o")
prompt = """
    You are an Organisation chatbot. Follow the below rules:
    1. When you get questions about employees and their reporting structure, call get_employees tool
    2. When you get questions about groceries, route the query to "grocery" agent
"""

tools = [get_employees]

organisation_chatbot_agent = Agent(prompt, tools, model)
organisation_chatbot_agent.create()
config = {"configurable": {"thread_id": "test-thread"}}

print(organisation_chatbot_agent.invoke([("user", "Who is Alice?")], config))
print(organisation_chatbot_agent.invoke([("user", "How many employees are reporting to Alice?")], config))
print(organisation_chatbot_agent.invoke([("user", "How many developers, analysts and designers?")], config))

from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings


class LLMRAGBasedNavigator:
    def __init__(self):
        self.__model = ChatOpenAI()
        self.__navigator_prompt_template = template = """
                    Provide the location of the product for the given customer's query in the Question
                    1. Classify the product requested in the customer's query under one of the following categories:
                        - Vegetables & Fruits
                        - Cereals
                        - Milk products
                        - Household cleaning
                        - Baby products
                        - Chocolates
                    2. Based on the category, follow the below location instructions and identify the location of the product:
                    {context}
                    3. Generate the answer in the format of json.

                    Question: {question}
                  """

    def __create_retreiver(self, context):
        self.__vectorstore = FAISS.from_texts(context, embedding=OpenAIEmbeddings())
        self.__retriever = self.__vectorstore.as_retriever()

    def __create_prompt(self):
        self.__prompt = ChatPromptTemplate.from_template(self.__navigator_prompt_template)

    def create_llm_chat_context(self, context):
        self.__create_retreiver(context)
        self.__create_prompt()
        self.__llm_chain = (
            {"context": self.__retriever, "question": RunnablePassthrough()}
            | self.__prompt
            | self.__model
            | StrOutputParser()
        )

    def chat_with_llm(self, question):
        response = self.__llm_chain.invoke(question)
        return response

context_for_store_navigation =  ["Vegetables & Fruits are in VF-Section A",
                                    "Cereals are in Cereals-Section B",
                                    "Milk products are in Milk products-Section C",
                                    "Household cleaning are in Household-Section D",
                                    "Baby products are in Baby-Section E",
                                    "Chocolates are in Sweets-Section F"]
llm_navigator = LLMRAGBasedNavigator()
llm_navigator.create_llm_chat_context(context_for_store_navigation)

llm_navigator.chat_with_llm("where is onion?")

from typing import Annotated, Any, Dict, Optional, Sequence, TypedDict, List, Tuple
from langchain_core.messages import BaseMessage, HumanMessage

class AgentState(TypedDict):
    query: Sequence[BaseMessage]
    result: str

def org_chatbot(state):
    print(f'Org agent:')
    print(state)
    print("....")
    query = state['query']
    organisation_chatbot_agent = Agent(prompt, tools, model)
    organisation_chatbot_agent.create()
    config = {"configurable": {"thread_id": "test-thread"}}
    result = organisation_chatbot_agent.invoke([("user", query)], config)
    return {'result': result}

# todo: rename
def route(state):
    result = state['result']
    print(result)
    if "grocery" in result:
        return "grocery"
    else:
        return END

def grocery(state):
    print(f'Grocery agent:')
    query = state['query']
    result = llm_navigator.chat_with_llm(query)
    return {'result':result}

from langgraph.graph import END, StateGraph
workflow = StateGraph(AgentState)

# Define the nodes
workflow.add_node("org_chatbot", org_chatbot)
workflow.add_node("grocery", grocery)

# Build graph
workflow.set_entry_point("org_chatbot")
workflow.add_conditional_edges("org_chatbot", route)
workflow.add_edge("org_chatbot", END)

app = workflow.compile()

from langchain_core.messages import HumanMessage

#todo: continuous chat with interactive input and convert to HR agent
for stream_msg in app.stream({"query":"How many employees are reporting to Alice?"}):
    if "__end__" not in stream_msg:
        if "org_chatbot" in stream_msg:
          print(stream_msg["org_chatbot"])
        elif "grocery" in stream_msg:
          print(stream_msg["grocery"])
        print("----")

for stream_msg in app.stream({"query":"where is onion?"}):
    if "__end__" not in stream_msg:
        if "org_chatbot" in stream_msg:
          print(stream_msg["org_chatbot"])
        elif "grocery" in stream_msg:
          print(stream_msg["grocery"])
        print("----")